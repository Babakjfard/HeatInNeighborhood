---
title: "Heat In My Neighborhood"
output:
  html_document:
    df_print: paged
---

This is a test model to calculate the ambient temperatures in different parts of an urban area. As a show case and proof of concept, in the first step it uses data from use [AoT network data](http://arrayofthings.github.io/) to build a Machine Learning train/test model. AoT project is expected to grow to [other cities](https://aot-file-browser.plenar.io/data-sets) throughouth the U.S., thereofre the model can be better generalized in a year with data available for at least 9 other urban areas. 

The predicted temperatures can help Urban Planners, City Officials, healthcare providers and other related institutions to have a district level map of temperatures changes inside a city, therefore planning for the related matters, such as heatwave adaptation and mitigation plans.

## The Approach
The problem can be broken down into two problems as follow:

### 1) A predictive model for calculating ambient temperature
A Machine Learning algorithm (ML) considering different potential features in predicting the ambient temperature. Currently, these features are considered for training and testing the model:

* Ambient temperatures form AoT network. (The predicted variable)
* Land Surface Temperature (LST): to be calculated from Remote Sensing data. (30m x 30m resolution)
* Population: Can be extracted from [American Community Survey - ACS  Data](https://www.census.gov/programs-surveys/acs/data.html). (Census Block Group level)
* Daytime population from ACS data. (Census Block Group level)
* Percent of imprevious from [National Land Cover Database](https://www.mrlc.gov/)
* Lancover Type from [National Land Cover Database](https://www.mrlc.gov/)

### 2) A downscaling model for daily LST values 
One problem is that the available remote sensing data for LST (from LandSat) in 30x30m resolution are available in 16 days time periods. Therefore a model created based on this data is not trustworthy. On ther hand other there are daily remote sensing data that can be used for calculating LST but usually in coarser spacial reolution (MODIS in 1 km resolution). A deep learning model is suggested to be trained from timely overlapped datasets of both remote sensing and to scale down the 1 km LST into 30x30m resolution. The model will be trained and validated from the overlapped (16 days data) parts and then be used to change 1x1 km resolution daily LST values into 30x30 m  values for the study period. 
The suggested method is to do [Transfer Learning](http://cs231n.github.io/transfer-learning/) on a [pre-trained deep learning model](https://arxiv.org/abs/1703.03126) by the previous Ph.D. candidate at SDS lab, [Thoms J.Vandal](http://www1.coe.neu.edu/~tvandal/).  

The final product will be a web-app that user (an urban planner, a healthcare provider, or anyone who is interested about ambient temperature highs in the city) can click a point insidet the city and receive predictions about the ambient temperature changes in that part, and a comparison to other places of the city. The initial model will focus to the city of Chicago, as explianed below.

# Proof of Concept
Following shows the steps used for gathering the data and creating a preliminary machine learning model.

## Mapping Nodes on Block Groups
Here there is a basic map showing the locations of AoT nodes in Chicago and the ACS Census Blocks with their total populations
```{r }
library(mapview)
library(RColorBrewer)

cols <- colorRampPalette(brewer.pal(20, "Reds"))
# Blocks
m1 <- mapview(chicago, z="estimate", popup=popupTable(chicago, zcol = c("estimate", "moe")),
              co.regions=cols(40),cex="estimate",legend=FALSE)

# nodes
m2 <- mapview(nodes.spt, size=5, legend=TRUE, color="red", color.region="white", popup=popupTable(nodes.spt, zcol = c("address", "node_id")))

m1+m2
```

It's look like the available nodes do not cover all the Census Block. To reduce the area of study, Iâ€™ll add Chicago community areas as a layer. 

```{r }
# First to remove the outlier node!
outlier.index <- which.min(nodes.spt$lon)
nodes.spt <- nodes.spt[-outlier.index,]
Stations <- Stations[-outlier.index,]
temp.daily <- temp.daily[,,-outlier.index]


# Now to eliminate BGs outside the containing box
this.area <-st_sfc(st_polygon(list(rbind(c(min(nodes.spt$lon), min(nodes.spt$lat)),
                             c(max(nodes.spt$lon), min(nodes.spt$lat)),
                             c(max(nodes.spt$lon), max(nodes.spt$lat)),
                             c(min(nodes.spt$lon), max(nodes.spt$lat)),
                             c(min(nodes.spt$lon), min(nodes.spt$lat)))))) %>% st_sf(relation_to_geometry = NA_character_)

st_crs(this.area) <- st_crs(chicago)
#st_crs(this.area) <- "+proj=longlat +datum=NAD83 +no_defs"
new_chicago <- st_intersection(this.area, chicago)
m2 <- mapview(nodes.spt, size=5, legend=FALSE, color="red", popup=popupTable(nodes.spt, zcol = c("address", "node_id")))

m3 <- mapview(new_chicago, z="estimate", popup=popupTable(chicago, zcol = c("estimate", "moe")),
              co.regions=cols(40),cex="estimate",legend=FALSE)
m2 + m3
```
## Getting LandSat 8 data and calculate Land Surface Temperature (LST)
The Landsat is a multispectral satellite currently on the eighth of it's series. Landsat-8 data is freely available on the USGS's Earth Explorer website. All we need to do is sign up and find a scene that match our area of study. The notation used to catalog Landsat-8 images is called [Worldwide Reference System 2 (WRS-2)](https://landsat.usgs.gov/what-worldwide-reference-system-wrs). The Landsat follows the same paths imaging the earth every 16 days. Each path is split into multiple rows. So, each scene has a path and a row. 16 days later, another scene will have the same path and row than the previous scene. This is the essence of the WRS-2 system.

USGS provides [Shape files](https://landsat.usgs.gov/pathrow-shapefiles) of these paths and rows that let us quickly visualize, interact and select the important images. After checking the shape file of Landsat 4-8, WRS-2, I found that city of Chicago is within Path 23 and row 31. ( or can use a converter to find Path and Row from latitude and longitude).

at this time I used GloVis tool to download 6 images of Landsat 8 OLI/TIRS C1 Level-1. Then to test the model and for the proof of concept I am just adding one layer (band 6) of downloaded LandSat GeoTiff Image.
The following figure shows the extracted LandSat image of Band6 containing Chicago. 

These are used to calculate LST values and pick the values for each 30x30m tile in the city.
```{r }
source("LandSat_band6.R")
mapviewOptions(mapview.maxpixels = 5e+05)
m4 <- mapview(landsat, legend=FALSE)

m4+m2+m3
```

## Data Cleaning and Exploratory Data Analysis
In this section a part (one day) of data are extracted and an R dataframe is created. The heading of the dataframe looks like below. It contains one day of data for the location of each of the previouosly mapped sensor nodes

```{r }
load("validation_test.rda")
head(dataset, n=10)
```

And a summary of the dataset:
```{r }
summary(dataset)
```

### visual comaprisons of data. 
Boxplots:
```{r }
library(ggplot2)
library(plotly)
library(tidyverse)

# tidy the data for visualization
data.tidy <- dataset[,1:7] %>% gather(key = "parameter", value = "value", -c(lat, lon))
bxp.tmeps <- ggplot(data.tidy, aes(x=parameter, y=value))+
  geom_boxplot()+facet_wrap(~parameter, scales = "free_y")
bxp.tmeps <- ggplotly(bxp.tmeps)
bxp.tmeps
```

Histograms:
```{r }
hist.temps <- ggplot(data.tidy, aes(value))+
  geom_histogram()+facet_wrap(~parameter, scales = "free_x")
h <- ggplotly(hist.temps)
h
```

Correlation plot for the variables

```{r , results='hide'}
library(corrplot)
data_to_corr <- dataset[, -c(2,3)]
data_to_corr$Land_Cover <- as.numeric(data_to_corr$Land_Cover)
correlations <- cor(data_to_corr)
corrplot(correlations, method="circle")
```


## Training the models and comparison
As it is a sample of a regression problem, the follwing models are consider for creating the predictive models.

* Linear Regression (LR)
* Generalized Linear Model (GLM)
* Support Vector Machine (SVM)
* Classification and Regression Tree (CART)
* As an ensemble model a Random Forest (RF) model is also considered and added.

The data was randomly assigned into 80% trainng and 20% validation sets. Then all data were normalized before running the models, and a 10-fold cross validation method was used for fitting the models. The following diagram shows the evaluation of each of the methods based on three evaluation metrics, Mean Absolute Error (MAE), Root of Mean Squared Error (RMSE), and Rsquared.

```{r , results='hide'}
dotplot(results_with_ensembles)
```

This is just a showcase of one part of the suggested model, and the used data may not be accurate at this step.